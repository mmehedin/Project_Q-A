{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/\n",
    "#https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attentionLSTM import AttentionLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# source text\n",
    "data1 = \"\"\" Jack and Jill went up the hill\\n\n",
    "            To fetch a pail of water\\n\n",
    "        Jack fell down and broke his crown\\n\n",
    "        And Jill came tumbling after\\n \"\"\"\n",
    "\n",
    "\n",
    "data = \"\"\" Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer.fit_on_texts([data])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21,\n",
       "  1,\n",
       "  22,\n",
       "  23,\n",
       "  3,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  1,\n",
       "  5,\n",
       "  27,\n",
       "  10,\n",
       "  11,\n",
       "  4,\n",
       "  3,\n",
       "  28,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  12,\n",
       "  8,\n",
       "  13,\n",
       "  9,\n",
       "  29,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  14,\n",
       "  6,\n",
       "  30,\n",
       "  15,\n",
       "  4,\n",
       "  3,\n",
       "  31,\n",
       "  7,\n",
       "  2,\n",
       "  32,\n",
       "  16,\n",
       "  33,\n",
       "  34,\n",
       "  16,\n",
       "  1,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  17,\n",
       "  1,\n",
       "  5,\n",
       "  14,\n",
       "  4,\n",
       "  1,\n",
       "  18,\n",
       "  2,\n",
       "  1,\n",
       "  41,\n",
       "  42,\n",
       "  13,\n",
       "  43,\n",
       "  1,\n",
       "  18,\n",
       "  4,\n",
       "  1,\n",
       "  19,\n",
       "  3,\n",
       "  44,\n",
       "  45,\n",
       "  2,\n",
       "  46,\n",
       "  6,\n",
       "  47,\n",
       "  15,\n",
       "  4,\n",
       "  3,\n",
       "  48,\n",
       "  2,\n",
       "  1,\n",
       "  19,\n",
       "  20,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  1,\n",
       "  12,\n",
       "  8,\n",
       "  52,\n",
       "  53,\n",
       "  17,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  9,\n",
       "  57,\n",
       "  20,\n",
       "  1,\n",
       "  58,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  59,\n",
       "  6,\n",
       "  9,\n",
       "  3,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  6,\n",
       "  1,\n",
       "  10,\n",
       "  11,\n",
       "  4,\n",
       "  3,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  7,\n",
       "  2,\n",
       "  8]]"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21,\n",
       " 1,\n",
       " 22,\n",
       " 23,\n",
       " 3,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 1,\n",
       " 5,\n",
       " 27,\n",
       " 10,\n",
       " 11,\n",
       " 4,\n",
       " 3,\n",
       " 28,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 12,\n",
       " 8,\n",
       " 13,\n",
       " 9,\n",
       " 29,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 14,\n",
       " 6,\n",
       " 30,\n",
       " 15,\n",
       " 4,\n",
       " 3,\n",
       " 31,\n",
       " 7,\n",
       " 2,\n",
       " 32,\n",
       " 16,\n",
       " 33,\n",
       " 34,\n",
       " 16,\n",
       " 1,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 17,\n",
       " 1,\n",
       " 5,\n",
       " 14,\n",
       " 4,\n",
       " 1,\n",
       " 18,\n",
       " 2,\n",
       " 1,\n",
       " 41,\n",
       " 42,\n",
       " 13,\n",
       " 43,\n",
       " 1,\n",
       " 18,\n",
       " 4,\n",
       " 1,\n",
       " 19,\n",
       " 3,\n",
       " 44,\n",
       " 45,\n",
       " 2,\n",
       " 46,\n",
       " 6,\n",
       " 47,\n",
       " 15,\n",
       " 4,\n",
       " 3,\n",
       " 48,\n",
       " 2,\n",
       " 1,\n",
       " 19,\n",
       " 20,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 1,\n",
       " 12,\n",
       " 8,\n",
       " 52,\n",
       " 53,\n",
       " 17,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 9,\n",
       " 57,\n",
       " 20,\n",
       " 1,\n",
       " 58,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 59,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 6,\n",
       " 1,\n",
       " 10,\n",
       " 11,\n",
       " 4,\n",
       " 3,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 7,\n",
       " 2,\n",
       " 8]"
      ]
     },
     "execution_count": 1014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'a': 3,\n",
       " 'is': 4,\n",
       " 'main': 5,\n",
       " 'and': 6,\n",
       " 'statue': 7,\n",
       " 'mary': 8,\n",
       " 'in': 9,\n",
       " 'gold': 10,\n",
       " 'dome': 11,\n",
       " 'virgin': 12,\n",
       " 'immediately': 13,\n",
       " 'building': 14,\n",
       " 'it': 15,\n",
       " 'with': 16,\n",
       " 'to': 17,\n",
       " 'basilica': 18,\n",
       " 'grotto': 19,\n",
       " 'at': 20,\n",
       " 'architecturally': 21,\n",
       " 'school': 22,\n",
       " 'has': 23,\n",
       " 'catholic': 24,\n",
       " 'character': 25,\n",
       " 'atop': 26,\n",
       " \"building's\": 27,\n",
       " 'golden': 28,\n",
       " 'front': 29,\n",
       " 'facing': 30,\n",
       " 'copper': 31,\n",
       " 'christ': 32,\n",
       " 'arms': 33,\n",
       " 'upraised': 34,\n",
       " 'legend': 35,\n",
       " 'venite': 36,\n",
       " 'ad': 37,\n",
       " 'me': 38,\n",
       " 'omnes': 39,\n",
       " 'next': 40,\n",
       " 'sacred': 41,\n",
       " 'heart': 42,\n",
       " 'behind': 43,\n",
       " 'marian': 44,\n",
       " 'place': 45,\n",
       " 'prayer': 46,\n",
       " 'reflection': 47,\n",
       " 'replica': 48,\n",
       " 'lourdes': 49,\n",
       " 'france': 50,\n",
       " 'where': 51,\n",
       " 'reputedly': 52,\n",
       " 'appeared': 53,\n",
       " 'saint': 54,\n",
       " 'bernadette': 55,\n",
       " 'soubirous': 56,\n",
       " '1858': 57,\n",
       " 'end': 58,\n",
       " 'drive': 59,\n",
       " 'direct': 60,\n",
       " 'line': 61,\n",
       " 'that': 62,\n",
       " 'connects': 63,\n",
       " 'through': 64,\n",
       " '3': 65,\n",
       " 'statues': 66,\n",
       " 'simple': 67,\n",
       " 'modern': 68,\n",
       " 'stone': 69}"
      ]
     },
     "execution_count": 1015,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 123\n"
     ]
    }
   ],
   "source": [
    "#create sequences\n",
    "sequences=list()\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, 1],\n",
       " [1, 22],\n",
       " [22, 23],\n",
       " [23, 3],\n",
       " [3, 24],\n",
       " [24, 25],\n",
       " [25, 26],\n",
       " [26, 1],\n",
       " [1, 5],\n",
       " [5, 27],\n",
       " [27, 10],\n",
       " [10, 11],\n",
       " [11, 4],\n",
       " [4, 3],\n",
       " [3, 28],\n",
       " [28, 7],\n",
       " [7, 2],\n",
       " [2, 1],\n",
       " [1, 12],\n",
       " [12, 8],\n",
       " [8, 13],\n",
       " [13, 9],\n",
       " [9, 29],\n",
       " [29, 2],\n",
       " [2, 1],\n",
       " [1, 5],\n",
       " [5, 14],\n",
       " [14, 6],\n",
       " [6, 30],\n",
       " [30, 15],\n",
       " [15, 4],\n",
       " [4, 3],\n",
       " [3, 31],\n",
       " [31, 7],\n",
       " [7, 2],\n",
       " [2, 32],\n",
       " [32, 16],\n",
       " [16, 33],\n",
       " [33, 34],\n",
       " [34, 16],\n",
       " [16, 1],\n",
       " [1, 35],\n",
       " [35, 36],\n",
       " [36, 37],\n",
       " [37, 38],\n",
       " [38, 39],\n",
       " [39, 40],\n",
       " [40, 17],\n",
       " [17, 1],\n",
       " [1, 5],\n",
       " [5, 14],\n",
       " [14, 4],\n",
       " [4, 1],\n",
       " [1, 18],\n",
       " [18, 2],\n",
       " [2, 1],\n",
       " [1, 41],\n",
       " [41, 42],\n",
       " [42, 13],\n",
       " [13, 43],\n",
       " [43, 1],\n",
       " [1, 18],\n",
       " [18, 4],\n",
       " [4, 1],\n",
       " [1, 19],\n",
       " [19, 3],\n",
       " [3, 44],\n",
       " [44, 45],\n",
       " [45, 2],\n",
       " [2, 46],\n",
       " [46, 6],\n",
       " [6, 47],\n",
       " [47, 15],\n",
       " [15, 4],\n",
       " [4, 3],\n",
       " [3, 48],\n",
       " [48, 2],\n",
       " [2, 1],\n",
       " [1, 19],\n",
       " [19, 20],\n",
       " [20, 49],\n",
       " [49, 50],\n",
       " [50, 51],\n",
       " [51, 1],\n",
       " [1, 12],\n",
       " [12, 8],\n",
       " [8, 52],\n",
       " [52, 53],\n",
       " [53, 17],\n",
       " [17, 54],\n",
       " [54, 55],\n",
       " [55, 56],\n",
       " [56, 9],\n",
       " [9, 57],\n",
       " [57, 20],\n",
       " [20, 1],\n",
       " [1, 58],\n",
       " [58, 2],\n",
       " [2, 1],\n",
       " [1, 5],\n",
       " [5, 59],\n",
       " [59, 6],\n",
       " [6, 9],\n",
       " [9, 3],\n",
       " [3, 60],\n",
       " [60, 61],\n",
       " [61, 62],\n",
       " [62, 63],\n",
       " [63, 64],\n",
       " [64, 65],\n",
       " [65, 66],\n",
       " [66, 6],\n",
       " [6, 1],\n",
       " [1, 10],\n",
       " [10, 11],\n",
       " [11, 4],\n",
       " [4, 3],\n",
       " [3, 67],\n",
       " [67, 68],\n",
       " [68, 69],\n",
       " [69, 7],\n",
       " [7, 2],\n",
       " [2, 8]]"
      ]
     },
     "execution_count": 1019,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y elements\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,0],sequences[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21,  1, 22, 23,  3, 24, 25, 26,  1,  5, 27, 10, 11,  4,  3, 28,  7,\n",
       "        2,  1, 12,  8, 13,  9, 29,  2,  1,  5, 14,  6, 30, 15,  4,  3, 31,\n",
       "        7,  2, 32, 16, 33, 34, 16,  1, 35, 36, 37, 38, 39, 40, 17,  1,  5,\n",
       "       14,  4,  1, 18,  2,  1, 41, 42, 13, 43,  1, 18,  4,  1, 19,  3, 44,\n",
       "       45,  2, 46,  6, 47, 15,  4,  3, 48,  2,  1, 19, 20, 49, 50, 51,  1,\n",
       "       12,  8, 52, 53, 17, 54, 55, 56,  9, 57, 20,  1, 58,  2,  1,  5, 59,\n",
       "        6,  9,  3, 60, 61, 62, 63, 64, 65, 66,  6,  1, 10, 11,  4,  3, 67,\n",
       "       68, 69,  7,  2])"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 22, 23,  3, 24, 25, 26,  1,  5, 27, 10, 11,  4,  3, 28,  7,  2,\n",
       "        1, 12,  8, 13,  9, 29,  2,  1,  5, 14,  6, 30, 15,  4,  3, 31,  7,\n",
       "        2, 32, 16, 33, 34, 16,  1, 35, 36, 37, 38, 39, 40, 17,  1,  5, 14,\n",
       "        4,  1, 18,  2,  1, 41, 42, 13, 43,  1, 18,  4,  1, 19,  3, 44, 45,\n",
       "        2, 46,  6, 47, 15,  4,  3, 48,  2,  1, 19, 20, 49, 50, 51,  1, 12,\n",
       "        8, 52, 53, 17, 54, 55, 56,  9, 57, 20,  1, 58,  2,  1,  5, 59,  6,\n",
       "        9,  3, 60, 61, 62, 63, 64, 65, 66,  6,  1, 10, 11,  4,  3, 67, 68,\n",
       "       69,  7,  2,  8])"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 1024,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_vector = np.ones(22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "global model\n",
    "model =Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size, 10, input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(AttentionLSTM(150, embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi=Bidirectional(LSTM(50, return_sequences=True),input_shape=(2, 1), merge_mode='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(50))\n",
    "\n",
    "#model.add(bi)\n",
    "#print(bi.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.shape)\n",
    "den=Dense(vocab_size, activation='softmax')\n",
    "#den = TimeDistributed(Dense(20, activation='sigmoid')) #need to flatten\n",
    "#model.add(AttentionDecoder(150, n_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50)\n"
     ]
    }
   ],
   "source": [
    "model.add(den)\n",
    "print(den.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 1, 10)             700       \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 70)                3570      \n",
      "=================================================================\n",
      "Total params: 16,470\n",
      "Trainable params: 16,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss():\n",
    "    loss=list()\n",
    "    hist =model.fit(X,y, epochs=500, verbose=2)  \n",
    "    #loss.append(hist.history['loss'][0])\n",
    "    for x in hist.history['loss']:\n",
    "        loss.append(x)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 14s - loss: 0.0749 - acc: 0.9857\n",
      "Epoch 2/500\n",
      " - 0s - loss: 0.0748 - acc: 0.9857\n",
      "Epoch 3/500\n",
      " - 0s - loss: 0.0748 - acc: 0.9857\n",
      "Epoch 4/500\n",
      " - 0s - loss: 0.0748 - acc: 0.9857\n",
      "Epoch 5/500\n",
      " - 0s - loss: 0.0747 - acc: 0.9857\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.0747 - acc: 0.9857\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.0746 - acc: 0.9857\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.0746 - acc: 0.9857\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.0745 - acc: 0.9857\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.0745 - acc: 0.9857\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.0744 - acc: 0.9857\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.0744 - acc: 0.9857\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.0743 - acc: 0.9857\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.0742 - acc: 0.9857\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.0742 - acc: 0.9857\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.0741 - acc: 0.9857\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.0740 - acc: 0.9857\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.0739 - acc: 0.9857\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.0738 - acc: 0.9857\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.0737 - acc: 0.9857\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.0735 - acc: 0.9857\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.0734 - acc: 0.9857\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.0733 - acc: 0.9857\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.0731 - acc: 0.9857\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.0729 - acc: 0.9857\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.0727 - acc: 0.9857\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.0725 - acc: 0.9857\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.0722 - acc: 0.9857\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.0719 - acc: 0.9857\n",
      "Epoch 30/500\n",
      " - 0s - loss: 0.0717 - acc: 0.9857\n",
      "Epoch 31/500\n",
      " - 0s - loss: 0.0713 - acc: 0.9857\n",
      "Epoch 32/500\n",
      " - 0s - loss: 0.0710 - acc: 0.9857\n",
      "Epoch 33/500\n",
      " - 0s - loss: 0.0706 - acc: 0.9857\n",
      "Epoch 34/500\n",
      " - 0s - loss: 0.0702 - acc: 0.9857\n",
      "Epoch 35/500\n",
      " - 0s - loss: 0.0697 - acc: 0.9857\n",
      "Epoch 36/500\n",
      " - 0s - loss: 0.0693 - acc: 0.9857\n",
      "Epoch 37/500\n",
      " - 0s - loss: 0.0688 - acc: 0.9857\n",
      "Epoch 38/500\n",
      " - 0s - loss: 0.0682 - acc: 0.9857\n",
      "Epoch 39/500\n",
      " - 0s - loss: 0.0677 - acc: 0.9857\n",
      "Epoch 40/500\n",
      " - 0s - loss: 0.0671 - acc: 0.9857\n",
      "Epoch 41/500\n",
      " - 0s - loss: 0.0666 - acc: 0.9857\n",
      "Epoch 42/500\n",
      " - 0s - loss: 0.0659 - acc: 0.9857\n",
      "Epoch 43/500\n",
      " - 0s - loss: 0.0654 - acc: 0.9857\n",
      "Epoch 44/500\n",
      " - 0s - loss: 0.0648 - acc: 0.9857\n",
      "Epoch 45/500\n",
      " - 0s - loss: 0.0643 - acc: 0.9857\n",
      "Epoch 46/500\n",
      " - 0s - loss: 0.0639 - acc: 0.9857\n",
      "Epoch 47/500\n",
      " - 0s - loss: 0.0634 - acc: 0.9857\n",
      "Epoch 48/500\n",
      " - 0s - loss: 0.0630 - acc: 0.9857\n",
      "Epoch 49/500\n",
      " - 0s - loss: 0.0627 - acc: 0.9857\n",
      "Epoch 50/500\n",
      " - 0s - loss: 0.0624 - acc: 0.9857\n",
      "Epoch 51/500\n",
      " - 0s - loss: 0.0621 - acc: 0.9857\n",
      "Epoch 52/500\n",
      " - 0s - loss: 0.0618 - acc: 0.9857\n",
      "Epoch 53/500\n",
      " - 0s - loss: 0.0616 - acc: 0.9857\n",
      "Epoch 54/500\n",
      " - 0s - loss: 0.0613 - acc: 0.9857\n",
      "Epoch 55/500\n",
      " - 0s - loss: 0.0611 - acc: 0.9857\n",
      "Epoch 56/500\n",
      " - 0s - loss: 0.0609 - acc: 0.9857\n",
      "Epoch 57/500\n",
      " - 0s - loss: 0.0607 - acc: 0.9857\n",
      "Epoch 58/500\n",
      " - 0s - loss: 0.0604 - acc: 0.9857\n",
      "Epoch 59/500\n",
      " - 0s - loss: 0.0602 - acc: 0.9857\n",
      "Epoch 60/500\n",
      " - 0s - loss: 0.0600 - acc: 0.9857\n",
      "Epoch 61/500\n",
      " - 0s - loss: 0.0598 - acc: 0.9857\n",
      "Epoch 62/500\n",
      " - 0s - loss: 0.0596 - acc: 0.9857\n",
      "Epoch 63/500\n",
      " - 0s - loss: 0.0594 - acc: 0.9857\n",
      "Epoch 64/500\n",
      " - 0s - loss: 0.0592 - acc: 0.9857\n",
      "Epoch 65/500\n",
      " - 0s - loss: 0.0590 - acc: 0.9857\n",
      "Epoch 66/500\n",
      " - 0s - loss: 0.0587 - acc: 0.9857\n",
      "Epoch 67/500\n",
      " - 0s - loss: 0.0585 - acc: 0.9857\n",
      "Epoch 68/500\n",
      " - 0s - loss: 0.0583 - acc: 0.9857\n",
      "Epoch 69/500\n",
      " - 0s - loss: 0.0581 - acc: 0.9857\n",
      "Epoch 70/500\n",
      " - 0s - loss: 0.0579 - acc: 0.9857\n",
      "Epoch 71/500\n",
      " - 0s - loss: 0.0577 - acc: 0.9857\n",
      "Epoch 72/500\n",
      " - 0s - loss: 0.0575 - acc: 0.9857\n",
      "Epoch 73/500\n",
      " - 0s - loss: 0.0573 - acc: 0.9857\n",
      "Epoch 74/500\n",
      " - 0s - loss: 0.0571 - acc: 0.9857\n",
      "Epoch 75/500\n",
      " - 0s - loss: 0.0568 - acc: 0.9857\n",
      "Epoch 76/500\n",
      " - 0s - loss: 0.0566 - acc: 0.9857\n",
      "Epoch 77/500\n",
      " - 0s - loss: 0.0564 - acc: 0.9857\n",
      "Epoch 78/500\n",
      " - 0s - loss: 0.0562 - acc: 0.9857\n",
      "Epoch 79/500\n",
      " - 0s - loss: 0.0560 - acc: 0.9857\n",
      "Epoch 80/500\n",
      " - 0s - loss: 0.0558 - acc: 0.9857\n",
      "Epoch 81/500\n",
      " - 0s - loss: 0.0556 - acc: 0.9857\n",
      "Epoch 82/500\n",
      " - 0s - loss: 0.0553 - acc: 0.9857\n",
      "Epoch 83/500\n",
      " - 0s - loss: 0.0551 - acc: 0.9857\n",
      "Epoch 84/500\n",
      " - 0s - loss: 0.0549 - acc: 0.9857\n",
      "Epoch 85/500\n",
      " - 0s - loss: 0.0547 - acc: 0.9857\n",
      "Epoch 86/500\n",
      " - 0s - loss: 0.0544 - acc: 0.9857\n",
      "Epoch 87/500\n",
      " - 0s - loss: 0.0542 - acc: 0.9857\n",
      "Epoch 88/500\n",
      " - 0s - loss: 0.0540 - acc: 0.9857\n",
      "Epoch 89/500\n",
      " - 0s - loss: 0.0537 - acc: 0.9857\n",
      "Epoch 90/500\n",
      " - 0s - loss: 0.0535 - acc: 0.9857\n",
      "Epoch 91/500\n",
      " - 0s - loss: 0.0532 - acc: 0.9857\n",
      "Epoch 92/500\n",
      " - 0s - loss: 0.0530 - acc: 0.9857\n",
      "Epoch 93/500\n",
      " - 0s - loss: 0.0527 - acc: 0.9857\n",
      "Epoch 94/500\n",
      " - 0s - loss: 0.0524 - acc: 0.9857\n",
      "Epoch 95/500\n",
      " - 0s - loss: 0.0522 - acc: 0.9857\n",
      "Epoch 96/500\n",
      " - 0s - loss: 0.0519 - acc: 0.9857\n",
      "Epoch 97/500\n",
      " - 0s - loss: 0.0516 - acc: 0.9857\n",
      "Epoch 98/500\n",
      " - 0s - loss: 0.0514 - acc: 0.9857\n",
      "Epoch 99/500\n",
      " - 0s - loss: 0.0511 - acc: 0.9857\n",
      "Epoch 100/500\n",
      " - 0s - loss: 0.0508 - acc: 0.9857\n",
      "Epoch 101/500\n",
      " - 0s - loss: 0.0505 - acc: 0.9857\n",
      "Epoch 102/500\n",
      " - 0s - loss: 0.0502 - acc: 0.9857\n",
      "Epoch 103/500\n",
      " - 0s - loss: 0.0499 - acc: 0.9857\n",
      "Epoch 104/500\n",
      " - 0s - loss: 0.0496 - acc: 0.9857\n",
      "Epoch 105/500\n",
      " - 0s - loss: 0.0492 - acc: 0.9857\n",
      "Epoch 106/500\n",
      " - 0s - loss: 0.0489 - acc: 0.9857\n",
      "Epoch 107/500\n",
      " - 0s - loss: 0.0486 - acc: 0.9857\n",
      "Epoch 108/500\n",
      " - 0s - loss: 0.0483 - acc: 0.9857\n",
      "Epoch 109/500\n",
      " - 0s - loss: 0.0480 - acc: 0.9857\n",
      "Epoch 110/500\n",
      " - 0s - loss: 0.0476 - acc: 0.9857\n",
      "Epoch 111/500\n",
      " - 0s - loss: 0.0473 - acc: 0.9857\n",
      "Epoch 112/500\n",
      " - 0s - loss: 0.0470 - acc: 0.9857\n",
      "Epoch 113/500\n",
      " - 0s - loss: 0.0466 - acc: 0.9857\n",
      "Epoch 114/500\n",
      " - 0s - loss: 0.0463 - acc: 0.9859\n",
      "Epoch 115/500\n",
      " - 0s - loss: 0.0460 - acc: 0.9862\n",
      "Epoch 116/500\n",
      " - 0s - loss: 0.0457 - acc: 0.9862\n",
      "Epoch 117/500\n",
      " - 0s - loss: 0.0453 - acc: 0.9862\n",
      "Epoch 118/500\n",
      " - 0s - loss: 0.0450 - acc: 0.9863\n",
      "Epoch 119/500\n",
      " - 0s - loss: 0.0447 - acc: 0.9864\n",
      "Epoch 120/500\n",
      " - 0s - loss: 0.0443 - acc: 0.9865\n",
      "Epoch 121/500\n",
      " - 0s - loss: 0.0440 - acc: 0.9868\n",
      "Epoch 122/500\n",
      " - 0s - loss: 0.0436 - acc: 0.9869\n",
      "Epoch 123/500\n",
      " - 0s - loss: 0.0433 - acc: 0.9871\n",
      "Epoch 124/500\n",
      " - 0s - loss: 0.0430 - acc: 0.9871\n",
      "Epoch 125/500\n",
      " - 0s - loss: 0.0427 - acc: 0.9872\n",
      "Epoch 126/500\n",
      " - 0s - loss: 0.0423 - acc: 0.9873\n",
      "Epoch 127/500\n",
      " - 0s - loss: 0.0420 - acc: 0.9872\n",
      "Epoch 128/500\n",
      " - 0s - loss: 0.0417 - acc: 0.9875\n",
      "Epoch 129/500\n",
      " - 0s - loss: 0.0414 - acc: 0.9875\n",
      "Epoch 130/500\n",
      " - 0s - loss: 0.0410 - acc: 0.9875\n",
      "Epoch 131/500\n",
      " - 0s - loss: 0.0407 - acc: 0.9875\n",
      "Epoch 132/500\n",
      " - 0s - loss: 0.0404 - acc: 0.9875\n",
      "Epoch 133/500\n",
      " - 0s - loss: 0.0401 - acc: 0.9875\n",
      "Epoch 134/500\n",
      " - 0s - loss: 0.0398 - acc: 0.9877\n",
      "Epoch 135/500\n",
      " - 0s - loss: 0.0394 - acc: 0.9878\n",
      "Epoch 136/500\n",
      " - 0s - loss: 0.0391 - acc: 0.9878\n",
      "Epoch 137/500\n",
      " - 0s - loss: 0.0388 - acc: 0.9878\n",
      "Epoch 138/500\n",
      " - 0s - loss: 0.0385 - acc: 0.9879\n",
      "Epoch 139/500\n",
      " - 0s - loss: 0.0382 - acc: 0.9879\n",
      "Epoch 140/500\n",
      " - 0s - loss: 0.0379 - acc: 0.9880\n",
      "Epoch 141/500\n",
      " - 0s - loss: 0.0376 - acc: 0.9882\n",
      "Epoch 142/500\n",
      " - 0s - loss: 0.0373 - acc: 0.9882\n",
      "Epoch 143/500\n",
      " - 0s - loss: 0.0370 - acc: 0.9885\n",
      "Epoch 144/500\n",
      " - 0s - loss: 0.0367 - acc: 0.9886\n",
      "Epoch 145/500\n",
      " - 0s - loss: 0.0364 - acc: 0.9887\n",
      "Epoch 146/500\n",
      " - 0s - loss: 0.0361 - acc: 0.9889\n",
      "Epoch 147/500\n",
      " - 0s - loss: 0.0358 - acc: 0.9889\n",
      "Epoch 148/500\n",
      " - 0s - loss: 0.0355 - acc: 0.9889\n",
      "Epoch 149/500\n",
      " - 0s - loss: 0.0352 - acc: 0.9889\n",
      "Epoch 150/500\n",
      " - 0s - loss: 0.0349 - acc: 0.9889\n",
      "Epoch 151/500\n",
      " - 0s - loss: 0.0346 - acc: 0.9889\n",
      "Epoch 152/500\n",
      " - 0s - loss: 0.0344 - acc: 0.9889\n",
      "Epoch 153/500\n",
      " - 0s - loss: 0.0341 - acc: 0.9890\n",
      "Epoch 154/500\n",
      " - 0s - loss: 0.0338 - acc: 0.9890\n",
      "Epoch 155/500\n",
      " - 0s - loss: 0.0335 - acc: 0.9890\n",
      "Epoch 156/500\n",
      " - 0s - loss: 0.0333 - acc: 0.9892\n",
      "Epoch 157/500\n",
      " - 0s - loss: 0.0330 - acc: 0.9892\n",
      "Epoch 158/500\n",
      " - 0s - loss: 0.0327 - acc: 0.9892\n",
      "Epoch 159/500\n",
      " - 0s - loss: 0.0325 - acc: 0.9892\n",
      "Epoch 160/500\n",
      " - 0s - loss: 0.0322 - acc: 0.9892\n",
      "Epoch 161/500\n",
      " - 0s - loss: 0.0319 - acc: 0.9892\n",
      "Epoch 162/500\n",
      " - 0s - loss: 0.0317 - acc: 0.9892\n",
      "Epoch 163/500\n",
      " - 0s - loss: 0.0314 - acc: 0.9894\n",
      "Epoch 164/500\n",
      " - 0s - loss: 0.0312 - acc: 0.9894\n",
      "Epoch 165/500\n",
      " - 0s - loss: 0.0309 - acc: 0.9895\n",
      "Epoch 166/500\n",
      " - 0s - loss: 0.0307 - acc: 0.9897\n",
      "Epoch 167/500\n",
      " - 0s - loss: 0.0305 - acc: 0.9897\n",
      "Epoch 168/500\n",
      " - 0s - loss: 0.0302 - acc: 0.9898\n",
      "Epoch 169/500\n",
      " - 0s - loss: 0.0300 - acc: 0.9900\n",
      "Epoch 170/500\n",
      " - 0s - loss: 0.0297 - acc: 0.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500\n",
      " - 0s - loss: 0.0295 - acc: 0.9900\n",
      "Epoch 172/500\n",
      " - 0s - loss: 0.0293 - acc: 0.9900\n",
      "Epoch 173/500\n",
      " - 0s - loss: 0.0290 - acc: 0.9900\n",
      "Epoch 174/500\n",
      " - 0s - loss: 0.0288 - acc: 0.9901\n",
      "Epoch 175/500\n",
      " - 0s - loss: 0.0286 - acc: 0.9901\n",
      "Epoch 176/500\n",
      " - 0s - loss: 0.0284 - acc: 0.9901\n",
      "Epoch 177/500\n",
      " - 0s - loss: 0.0282 - acc: 0.9901\n",
      "Epoch 178/500\n",
      " - 0s - loss: 0.0280 - acc: 0.9901\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.0278 - acc: 0.9901\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.0276 - acc: 0.9902\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.0274 - acc: 0.9904\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.0272 - acc: 0.9905\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.0270 - acc: 0.9906\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.0268 - acc: 0.9906\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.0266 - acc: 0.9906\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.0264 - acc: 0.9907\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.0262 - acc: 0.9907\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.0260 - acc: 0.9907\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.0258 - acc: 0.9907\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.0257 - acc: 0.9907\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.0255 - acc: 0.9907\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.0253 - acc: 0.9908\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.0252 - acc: 0.9908\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.0250 - acc: 0.9908\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.0248 - acc: 0.9908\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.0247 - acc: 0.9909\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.0245 - acc: 0.9911\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.0244 - acc: 0.9912\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.0242 - acc: 0.9912\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.0241 - acc: 0.9912\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.0239 - acc: 0.9913\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.0238 - acc: 0.9914\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.0236 - acc: 0.9915\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.0235 - acc: 0.9915\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.0234 - acc: 0.9915\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.0232 - acc: 0.9915\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.0231 - acc: 0.9915\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.0230 - acc: 0.9915\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.0229 - acc: 0.9915\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.0227 - acc: 0.9915\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.0226 - acc: 0.9919\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.0225 - acc: 0.9919\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.0224 - acc: 0.9919\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.0222 - acc: 0.9919\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.0221 - acc: 0.9922\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.0220 - acc: 0.9922\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.0219 - acc: 0.9922\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.0218 - acc: 0.9922\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.0217 - acc: 0.9922\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.0216 - acc: 0.9922\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.0215 - acc: 0.9922\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.0214 - acc: 0.9922\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.0213 - acc: 0.9921\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.0212 - acc: 0.9922\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.0211 - acc: 0.9922\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.0210 - acc: 0.9922\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.0209 - acc: 0.9923\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.0208 - acc: 0.9923\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.0208 - acc: 0.9923\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.0207 - acc: 0.9925\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.0206 - acc: 0.9925\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.0205 - acc: 0.9925\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.0204 - acc: 0.9925\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.0203 - acc: 0.9925\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.0203 - acc: 0.9927\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.0202 - acc: 0.9927\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.0201 - acc: 0.9927\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.0200 - acc: 0.9927\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.0200 - acc: 0.9926\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.0199 - acc: 0.9927\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.0198 - acc: 0.9928\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.0198 - acc: 0.9928\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.0197 - acc: 0.9929\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.0197 - acc: 0.9929\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.0196 - acc: 0.9929\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.0195 - acc: 0.9929\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.0195 - acc: 0.9929\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.0194 - acc: 0.9929\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.0193 - acc: 0.9929\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.0193 - acc: 0.9929\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.0192 - acc: 0.9929\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.0192 - acc: 0.9929\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.0191 - acc: 0.9929\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.0191 - acc: 0.9930\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.0190 - acc: 0.9930\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.0190 - acc: 0.9930\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.0189 - acc: 0.9930\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.0189 - acc: 0.9930\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.0188 - acc: 0.9930\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.0188 - acc: 0.9930\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.0187 - acc: 0.9930\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.0187 - acc: 0.9930\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.0186 - acc: 0.9930\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.0186 - acc: 0.9930\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.0185 - acc: 0.9930\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.0185 - acc: 0.9930\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.0185 - acc: 0.9929\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.0184 - acc: 0.9930\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.0184 - acc: 0.9930\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.0184 - acc: 0.9930\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.0183 - acc: 0.9930\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.0183 - acc: 0.9929\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.0182 - acc: 0.9929\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.0182 - acc: 0.9929\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.0182 - acc: 0.9928\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.0181 - acc: 0.9928\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.0181 - acc: 0.9930\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.0180 - acc: 0.9930\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.0180 - acc: 0.9929\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.0180 - acc: 0.9929\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.0179 - acc: 0.9929\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.0179 - acc: 0.9930\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.0179 - acc: 0.9930\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.0178 - acc: 0.9930\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.0178 - acc: 0.9930\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.0178 - acc: 0.9930\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.0177 - acc: 0.9930\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.0177 - acc: 0.9930\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.0177 - acc: 0.9931\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.0177 - acc: 0.9930\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.0176 - acc: 0.9930\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.0176 - acc: 0.9930\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.0176 - acc: 0.9931\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.0175 - acc: 0.9931\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.0175 - acc: 0.9931\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.0175 - acc: 0.9931\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.0175 - acc: 0.9931\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.0174 - acc: 0.9931\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.0174 - acc: 0.9931\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.0174 - acc: 0.9931\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.0173 - acc: 0.9931\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.0173 - acc: 0.9931\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.0173 - acc: 0.9929\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.0173 - acc: 0.9930\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.0172 - acc: 0.9931\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.0172 - acc: 0.9931\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.0172 - acc: 0.9931\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.0172 - acc: 0.9931\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.0171 - acc: 0.9931\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.0171 - acc: 0.9931\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.0171 - acc: 0.9931\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.0171 - acc: 0.9930\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.0171 - acc: 0.9930\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.0170 - acc: 0.9931\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.0170 - acc: 0.9931\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.0170 - acc: 0.9931\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.0170 - acc: 0.9929\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.0169 - acc: 0.9931\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.0169 - acc: 0.9931\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.0169 - acc: 0.9931\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.0169 - acc: 0.9931\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.0169 - acc: 0.9931\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.0168 - acc: 0.9930\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.0168 - acc: 0.9931\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.0168 - acc: 0.9930\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.0168 - acc: 0.9931\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.0168 - acc: 0.9931\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.0167 - acc: 0.9931\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.0167 - acc: 0.9929\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.0167 - acc: 0.9931\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.0167 - acc: 0.9930\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.0167 - acc: 0.9931\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.0167 - acc: 0.9931\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.0167 - acc: 0.9930\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.0166 - acc: 0.9931\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.0166 - acc: 0.9931\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.0166 - acc: 0.9931\n",
      "Epoch 338/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.0166 - acc: 0.9930\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.0166 - acc: 0.9931\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.0166 - acc: 0.9931\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.0165 - acc: 0.9930\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.0165 - acc: 0.9931\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.0165 - acc: 0.9930\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.0165 - acc: 0.9931\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.0165 - acc: 0.9931\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.0165 - acc: 0.9931\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9931\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9931\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9931\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9931\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9930\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9931\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9931\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9927\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.0164 - acc: 0.9929\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9930\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9930\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9931\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9931\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9931\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9931\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9931\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9931\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.0163 - acc: 0.9930\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9930\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9929\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9928\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9930\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9930\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9928\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9929\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9929\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9931\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9931\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.0162 - acc: 0.9931\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9931\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9931\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9931\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9929\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9930\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9928\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9930\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9929\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9931\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9930\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9931\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.0161 - acc: 0.9931\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9930\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9930\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9931\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9930\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9930\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.0160 - acc: 0.9930\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9928\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9930\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9930\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9930\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9930\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9930\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9930\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9930\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9930\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.0159 - acc: 0.9931\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9930\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9930\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9929\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9929\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9930\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9930\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9929\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9930\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9929\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9930\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9930\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9929\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.0158 - acc: 0.9931\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9929\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9928\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9929\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9929\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9929\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9929\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9929\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9929\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9931\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9929\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9931\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9930\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.0157 - acc: 0.9930\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9930\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9930\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9929\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9931\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9931\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9931\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9931\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.0156 - acc: 0.9931\n"
     ]
    }
   ],
   "source": [
    "results = DataFrame()\n",
    "results['loss bi'] = get_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      loss bi\n",
      "0    0.074873\n",
      "1    0.074832\n",
      "2    0.074794\n",
      "3    0.074756\n",
      "4    0.074716\n",
      "5    0.074674\n",
      "6    0.074632\n",
      "7    0.074588\n",
      "8    0.074539\n",
      "9    0.074488\n",
      "10   0.074433\n",
      "11   0.074375\n",
      "12   0.074310\n",
      "13   0.074243\n",
      "14   0.074167\n",
      "15   0.074082\n",
      "16   0.073990\n",
      "17   0.073897\n",
      "18   0.073796\n",
      "19   0.073670\n",
      "20   0.073544\n",
      "21   0.073411\n",
      "22   0.073253\n",
      "23   0.073087\n",
      "24   0.072883\n",
      "25   0.072706\n",
      "26   0.072464\n",
      "27   0.072214\n",
      "28   0.071947\n",
      "29   0.071666\n",
      "..        ...\n",
      "470  0.015695\n",
      "471  0.015681\n",
      "472  0.015677\n",
      "473  0.015684\n",
      "474  0.015688\n",
      "475  0.015687\n",
      "476  0.015687\n",
      "477  0.015663\n",
      "478  0.015674\n",
      "479  0.015681\n",
      "480  0.015681\n",
      "481  0.015674\n",
      "482  0.015663\n",
      "483  0.015657\n",
      "484  0.015661\n",
      "485  0.015655\n",
      "486  0.015654\n",
      "487  0.015651\n",
      "488  0.015664\n",
      "489  0.015643\n",
      "490  0.015647\n",
      "491  0.015654\n",
      "492  0.015642\n",
      "493  0.015641\n",
      "494  0.015643\n",
      "495  0.015630\n",
      "496  0.015625\n",
      "497  0.015648\n",
      "498  0.015625\n",
      "499  0.015635\n",
      "\n",
      "[500 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary\n",
      "reputedly\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "in_text = 'Mary'\n",
    "print(in_text)\n",
    "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "encoded = np.array(encoded)\n",
    "yhat = model.predict_classes(encoded, verbose=0)\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_word(word):\n",
    "    encoded = tokenizer.texts_to_sequences([word])[0]\n",
    "    return np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word(encoded_word, model=model):\n",
    "    #print(model.summary())\n",
    "    yht = model.predict_classes(encoded_word, verbose = 0)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == yht:\n",
    "            return(word)\n",
    "    return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sequence(w, tokenizer, model=model, nwords=1):\n",
    "    predicted = w\n",
    "    for _ in range(nwords):\n",
    "        #encoded = get_encoded_word(w)\n",
    "        yht = model.predict_classes(get_encoded_word(w), verbose=0)\n",
    "        current =''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index ==yht:\n",
    "                current = word\n",
    "                break\n",
    "        predicted+=(\" \"+word)\n",
    "        w=current\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reputedly'"
      ]
     },
     "execution_count": 1048,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_word(get_encoded_word('Mary'), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mary reputedly appeared to the main building'"
      ]
     },
     "execution_count": 1049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_sequence('mary', tokenizer, model, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8])"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52])"
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8])"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenizer.texts_to_sequences([in_text])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('the', 1), ('of', 2), ('a', 3), ('is', 4), ('main', 5), ('and', 6), ('statue', 7), ('mary', 8), ('in', 9), ('gold', 10), ('dome', 11), ('virgin', 12), ('immediately', 13), ('building', 14), ('it', 15), ('with', 16), ('to', 17), ('basilica', 18), ('grotto', 19), ('at', 20), ('architecturally', 21), ('school', 22), ('has', 23), ('catholic', 24), ('character', 25), ('atop', 26), (\"building's\", 27), ('golden', 28), ('front', 29), ('facing', 30), ('copper', 31), ('christ', 32), ('arms', 33), ('upraised', 34), ('legend', 35), ('venite', 36), ('ad', 37), ('me', 38), ('omnes', 39), ('next', 40), ('sacred', 41), ('heart', 42), ('behind', 43), ('marian', 44), ('place', 45), ('prayer', 46), ('reflection', 47), ('replica', 48), ('lourdes', 49), ('france', 50), ('where', 51), ('reputedly', 52), ('appeared', 53), ('saint', 54), ('bernadette', 55), ('soubirous', 56), ('1858', 57), ('end', 58), ('drive', 59), ('direct', 60), ('line', 61), ('that', 62), ('connects', 63), ('through', 64), ('3', 65), ('statues', 66), ('simple', 67), ('modern', 68), ('stone', 69)])"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of']"
      ]
     },
     "execution_count": 1054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.texts_to_sequences(['jack'])\n",
    "tokenizer.sequences_to_texts([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pre-pad sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                #out_word = tokenizer.sequences_to_texts([[index]])[0] #this works too\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(seq) for seq in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? where the main building is a marian place of the main building is a marian place of the main building\n"
     ]
    }
   ],
   "source": [
    "#print(generate_seq(model, tokenizer, max_length-1, 'When did Jack fall?', 3))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_decoder import AttentionDecoder\n",
    "from attention_decoder import _time_distributed_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the encoder-decoder with attention model\n",
    "def attention_model(n_timesteps_in, n_features):\n",
    "    model = Sequential()\n",
    "    #model.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(150, return_sequences=True), input_shape=(n_timesteps, n_features), merge_mode=mode))\n",
    "    #model.add(AttentionDecoder(150, n_features))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate a model, return accuracy\n",
    "def train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features):\n",
    "    # train LSTM\n",
    "    for epoch in range(5000):\n",
    "        # generate new random sequence\n",
    "        X,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "        # fit model for one epoch on this sequence\n",
    "        model.fit(X, y, epochs=1, verbose=0)\n",
    "    # evaluate LSTM\n",
    "    total, correct = 100, 0\n",
    "    for _ in range(total):\n",
    "        X,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
    "        yhat = model.predict(X, verbose=0)\n",
    "        if array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
    "            correct += 1        \n",
    "    return float(correct)/float(total)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure problem\n",
    "n_features = vocab_size\n",
    "n_timesteps_in = 5\n",
    "n_timesteps_out = 2\n",
    "n_repeats = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Encoder-Decoder With Attention Model')\n",
    "results = list()\n",
    "for _ in range(n_repeats):\n",
    "\tmodel = attention_model(n_timesteps_in, n_features)\n",
    "\taccuracy = train_evaluate_model(model, n_timesteps_in, n_timesteps_out, n_features)\n",
    "\tresults.append(accuracy)\n",
    "\tprint(accuracy)\n",
    "print('Mean Accuracy: %.2f%%' % (sum(results)/float(n_repeats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "uniform_data = np.random.rand(10, 12)\n",
    "sns.heatmap(uniform_data)\n",
    "#sns.heatmap(np.asmatrix(data), np.asmatrix(generate_seq(model, tokenizer, max_length-1, 'When did Jack fall?', 3)))\n",
    "#sns.heatmap(np.asmatrix(data), np.asmatrix(data))\n",
    "#np.asanyarray(data)\n",
    "#encoded\n",
    "sns.heatmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://seaborn.pydata.org/generated/seaborn.heatmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sns.load_dataset(\"flights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(flights.head(10))\n",
    "flights = flights.pivot(\"month\", \"year\", \"passengers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
